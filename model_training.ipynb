{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using only complaint features to predict recall status...\n",
      "Training RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.9745935  0.97761953 0.97049847 0.97660224 0.982706  ]\n",
      "Mean CV accuracy: 0.9764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Test Accuracy: 0.9715\n",
      "Training HistGradientBoosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.97560976 0.97456765 0.97049847 0.97761953 0.98067141]\n",
      "Mean CV accuracy: 0.9758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoosting Test Accuracy: 0.9707\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:04:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:04:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:04:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:04:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:04:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.97357724 0.97558494 0.97151577 0.97761953 0.97863683]\n",
      "Mean CV accuracy: 0.9754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:04:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test Accuracy: 0.9748\n",
      "\n",
      "Top 20 Important Features:\n",
      "                            Feature  Importance\n",
      "486      products_ford productmodel    0.295018\n",
      "254         summary_complaint_model    0.046571\n",
      "427                 products_450 sd    0.037845\n",
      "339          summary_complaint_time    0.032946\n",
      "428                    products_550    0.025414\n",
      "168          summary_complaint_door    0.023427\n",
      "425                    products_450    0.019502\n",
      "169    summary_complaint_door latch    0.017787\n",
      "250         summary_complaint_metal    0.017620\n",
      "244       summary_complaint_manager    0.016575\n",
      "76             summary_complaint_04    0.013302\n",
      "235         summary_complaint_local    0.012531\n",
      "279     summary_complaint_passenger    0.012241\n",
      "303       summary_complaint_replace    0.012029\n",
      "57            Model_transit connect    0.011791\n",
      "183  summary_complaint_experiencing    0.011075\n",
      "403                   products_2015    0.009180\n",
      "515                products_mustang    0.008902\n",
      "286     summary_complaint_potential    0.008863\n",
      "229         summary_complaint_latch    0.008822\n",
      "\n",
      "RandomForest Results:\n",
      "Test Accuracy: 0.9715\n",
      "Cross-validation Accuracy: 0.9764 (±0.0040)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.94      0.96       418\n",
      "        True       0.97      0.99      0.98       811\n",
      "\n",
      "    accuracy                           0.97      1229\n",
      "   macro avg       0.97      0.96      0.97      1229\n",
      "weighted avg       0.97      0.97      0.97      1229\n",
      "\n",
      "Confusion Matrix:\n",
      "[[393  25]\n",
      " [ 10 801]]\n",
      "\n",
      "HistGradientBoosting Results:\n",
      "Test Accuracy: 0.9707\n",
      "Cross-validation Accuracy: 0.9758 (±0.0034)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.94      0.96       418\n",
      "        True       0.97      0.99      0.98       811\n",
      "\n",
      "    accuracy                           0.97      1229\n",
      "   macro avg       0.97      0.96      0.97      1229\n",
      "weighted avg       0.97      0.97      0.97      1229\n",
      "\n",
      "Confusion Matrix:\n",
      "[[392  26]\n",
      " [ 10 801]]\n",
      "\n",
      "XGBoost Results:\n",
      "Test Accuracy: 0.9748\n",
      "Cross-validation Accuracy: 0.9754 (±0.0026)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.94      0.96       418\n",
      "        True       0.97      0.99      0.98       811\n",
      "\n",
      "    accuracy                           0.97      1229\n",
      "   macro avg       0.98      0.97      0.97      1229\n",
      "weighted avg       0.97      0.97      0.97      1229\n",
      "\n",
      "Confusion Matrix:\n",
      "[[395  23]\n",
      " [  8 803]]\n",
      "\n",
      "Model saved successfully!\n",
      "\n",
      "Example of prediction with new complaint data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\xgboost\\data.py:575: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      predicted_recall  recall_probability\n",
      "4135                 1            0.999770\n",
      "1443                 1            0.993149\n",
      "236                  1            0.977395\n",
      "1637                 0            0.000913\n",
      "2589                 1            0.956420\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Custom transformer for date features\n",
    "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, date_columns):\n",
    "        self.date_columns = date_columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        result = pd.DataFrame(index=X_copy.index)\n",
    "        \n",
    "        for col in self.date_columns:\n",
    "            if col in X_copy.columns:\n",
    "                # Convert to datetime\n",
    "                X_copy[col] = pd.to_datetime(X_copy[col], errors='coerce')\n",
    "                \n",
    "                # Extract features\n",
    "                if not X_copy[col].isna().all():\n",
    "                    result[f'{col}_year'] = X_copy[col].dt.year\n",
    "                    result[f'{col}_month'] = X_copy[col].dt.month\n",
    "                    result[f'{col}_day'] = X_copy[col].dt.day\n",
    "                    result[f'{col}_dayofweek'] = X_copy[col].dt.dayofweek\n",
    "                    \n",
    "                    # Calculate days since a reference date\n",
    "                    result[f'{col}_days_since_2000'] = (X_copy[col] - pd.Timestamp('2000-01-01')).dt.days\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Custom transformer for text features\n",
    "class TextFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, text_columns, max_features=1000):\n",
    "        self.text_columns = text_columns\n",
    "        self.max_features = max_features\n",
    "        self.vectorizers = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        for col in self.text_columns:\n",
    "            if col in X_copy.columns:\n",
    "                vectorizer = TfidfVectorizer(\n",
    "                    max_features=self.max_features,\n",
    "                    stop_words='english',\n",
    "                    min_df=5,\n",
    "                    ngram_range=(1, 2)\n",
    "                )\n",
    "                # Fill NaN values with empty string\n",
    "                text_data = X_copy[col].fillna('')\n",
    "                vectorizer.fit(text_data)\n",
    "                self.vectorizers[col] = vectorizer\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        result = pd.DataFrame(index=X_copy.index)\n",
    "        \n",
    "        for col, vectorizer in self.vectorizers.items():\n",
    "            if col in X_copy.columns:\n",
    "                # Fill NaN values with empty string\n",
    "                text_data = X_copy[col].fillna('')\n",
    "                # Transform text to TF-IDF features\n",
    "                text_features = vectorizer.transform(text_data)\n",
    "                \n",
    "                # Convert sparse matrix to DataFrame\n",
    "                feature_names = [f'{col}_{name}' for name in vectorizer.get_feature_names_out()]\n",
    "                text_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "                    text_features,\n",
    "                    index=X_copy.index,\n",
    "                    columns=feature_names\n",
    "                )\n",
    "                \n",
    "                # Join with results\n",
    "                result = pd.concat([result, text_df], axis=1)\n",
    "                \n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "# Custom transformer for components features\n",
    "class ComponentsFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.component_encoder = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if 'components' in X.columns:\n",
    "            # Extract unique components from the components column\n",
    "            all_components = []\n",
    "            for comp_list in X['components'].dropna():\n",
    "                try:\n",
    "                    # Assuming components is stored as a string representation of a list\n",
    "                    if isinstance(comp_list, str):\n",
    "                        # Try to clean and split the string\n",
    "                        comp_list = comp_list.replace('[', '').replace(']', '').replace(\"'\", \"\")\n",
    "                        components = [c.strip() for c in comp_list.split(',')]\n",
    "                        all_components.extend(components)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Get unique components\n",
    "            unique_components = list(set(all_components))\n",
    "            self.component_encoder = {comp: i for i, comp in enumerate(unique_components)}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        result = pd.DataFrame(index=X_copy.index)\n",
    "        \n",
    "        if 'components' in X_copy.columns and self.component_encoder:\n",
    "            # Initialize component features with zeros\n",
    "            for comp in self.component_encoder:\n",
    "                result[f'component_{comp}'] = 0\n",
    "            \n",
    "            # Fill in component features\n",
    "            for idx, comp_list in X_copy['components'].dropna().items():\n",
    "                try:\n",
    "                    if isinstance(comp_list, str):\n",
    "                        comp_list = comp_list.replace('[', '').replace(']', '').replace(\"'\", \"\")\n",
    "                        components = [c.strip() for c in comp_list.split(',')]\n",
    "                        for comp in components:\n",
    "                            if comp in self.component_encoder:\n",
    "                                result.loc[idx, f'component_{comp}'] = 1\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Add a component count feature\n",
    "        if 'components' in X_copy.columns:\n",
    "            result['component_count'] = X_copy['components'].apply(\n",
    "                lambda x: len(str(x).split(',')) if pd.notna(x) else 0\n",
    "            )\n",
    "        \n",
    "        return result\n",
    "\n",
    "def preprocess_and_train_complaint_model(df, target_column='recall_status'):\n",
    "    \"\"\"\n",
    "    Preprocess and train a model using only complaint features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame containing the data\n",
    "    target_column : Name of the target column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Trained model, preprocessors, and evaluation results\n",
    "    \"\"\"\n",
    "    # Make a copy of the dataset\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Define complaint features to keep\n",
    "    complaint_features = [\n",
    "        'odiNumber', 'crash', 'fire', 'numberOfInjuries', 'numberOfDeaths', \n",
    "        'dateOfIncident', 'dateComplaintFiled', 'incident_filing_lag', \n",
    "        'components', 'summary_complaint', 'products', 'Model', 'ModelYear'\n",
    "    ]\n",
    "    \n",
    "    # Keep only complaint features and target\n",
    "    features_to_use = [col for col in complaint_features if col in data.columns]\n",
    "    features_to_use.append(target_column)\n",
    "    data = data[features_to_use]\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = ['dateOfIncident', 'dateComplaintFiled']\n",
    "    for col in date_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "    \n",
    "    # Handle incident_filing_lag missing values\n",
    "    if 'dateOfIncident' in data.columns and 'dateComplaintFiled' in data.columns:\n",
    "        # Calculate lag where missing\n",
    "        mask = data['incident_filing_lag'].isna() & ~data['dateOfIncident'].isna() & ~data['dateComplaintFiled'].isna()\n",
    "        data.loc[mask, 'incident_filing_lag'] = (data.loc[mask, 'dateComplaintFiled'] - \n",
    "                                               data.loc[mask, 'dateOfIncident']).dt.days\n",
    "    \n",
    "    # Create additional features\n",
    "    current_year = datetime.now().year\n",
    "    if 'ModelYear' in data.columns:\n",
    "        data['vehicle_age'] = current_year - data['ModelYear']\n",
    "    \n",
    "    # Create severity score\n",
    "    if 'numberOfInjuries' in data.columns and 'numberOfDeaths' in data.columns:\n",
    "        data['severity_score'] = data['numberOfInjuries'] + data['numberOfDeaths'] * 5\n",
    "    \n",
    "    # Define feature groups\n",
    "    numeric_features = [\n",
    "        'numberOfInjuries', 'numberOfDeaths', 'incident_filing_lag',\n",
    "        'vehicle_age', 'ModelYear'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = [\n",
    "        'Model'\n",
    "    ]\n",
    "    \n",
    "    text_features = [\n",
    "        'summary_complaint', 'products'\n",
    "    ]\n",
    "    \n",
    "    date_features = [\n",
    "        'dateOfIncident', 'dateComplaintFiled'\n",
    "    ]\n",
    "    \n",
    "    binary_features = [\n",
    "        'crash', 'fire'\n",
    "    ]\n",
    "    \n",
    "    # Ensure we only use features that exist in the data\n",
    "    numeric_features = [col for col in numeric_features if col in data.columns]\n",
    "    categorical_features = [col for col in categorical_features if col in data.columns]\n",
    "    text_features = [col for col in text_features if col in data.columns]\n",
    "    date_features = [col for col in date_features if col in data.columns]\n",
    "    binary_features = [col for col in binary_features if col in data.columns]\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Define preprocessing for numeric features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Define preprocessing for categorical features\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    # Create column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('bin', 'passthrough', binary_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    # Create specialized feature extractors\n",
    "    date_extractor = DateFeatureExtractor(date_columns=date_features)\n",
    "    text_extractor = TextFeatureExtractor(text_columns=text_features, max_features=300)\n",
    "    component_extractor = ComponentsFeatureExtractor()\n",
    "    \n",
    "    # Apply main preprocessing\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_test_preprocessed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Apply specialized extractors\n",
    "    X_train_dates = date_extractor.fit_transform(X_train)\n",
    "    X_test_dates = date_extractor.transform(X_test)\n",
    "    \n",
    "    X_train_text = text_extractor.fit_transform(X_train)\n",
    "    X_test_text = text_extractor.transform(X_test)\n",
    "\n",
    "    \n",
    "    X_train_components = component_extractor.fit_transform(X_train)\n",
    "    X_test_components = component_extractor.transform(X_test)\n",
    "    \n",
    "    # Get feature names from preprocessor\n",
    "    feature_names = []\n",
    "    for name, trans, cols in preprocessor.transformers_:\n",
    "        if name != 'remainder' and trans != 'drop':\n",
    "            if name == 'cat':\n",
    "                # Get feature names for categorical columns after one-hot encoding\n",
    "                cat_features = []\n",
    "                for i, col in enumerate(cols):\n",
    "                    try:\n",
    "                        cat_values = trans.named_steps['onehot'].categories_[i]\n",
    "                        cat_features.extend([f\"{col}_{val}\" for val in cat_values])\n",
    "                    except:\n",
    "                        # Handle case where a column might be empty\n",
    "                        pass\n",
    "                feature_names.extend(cat_features)\n",
    "            else:\n",
    "                # For numeric and binary columns, keep original names\n",
    "                feature_names.extend(cols)\n",
    "    \n",
    "    # Convert preprocessed data to DataFrames\n",
    "    X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, index=X_train.index, columns=feature_names)\n",
    "    X_test_preprocessed_df = pd.DataFrame(X_test_preprocessed, index=X_test.index, columns=feature_names)\n",
    "    \n",
    "    # Combine all features\n",
    "    X_train_final = pd.concat([\n",
    "        X_train_preprocessed_df, X_train_dates, X_train_text, \n",
    "        X_train_components\n",
    "    ], axis=1)\n",
    "    \n",
    "    X_test_final = pd.concat([\n",
    "        X_test_preprocessed_df, X_test_dates, X_test_text,\n",
    "        X_test_components\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Define models to try\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'HistGradientBoosting': HistGradientBoostingClassifier(random_state=42),\n",
    "        'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = {}\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(model, X_train_final, y_train, cv=cv, scoring='accuracy')\n",
    "        print(f\"Cross-validation scores: {cv_scores}\")\n",
    "        print(f\"Mean CV accuracy: {cv_scores.mean():.4f}\")\n",
    "        \n",
    "        # Train on the full training set\n",
    "        model.fit(X_train_final, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_final)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'cv_accuracy': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'report': classification_report(y_test, y_pred),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Test Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "    \n",
    "    # Feature importance for the best model (if available)\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importances = pd.DataFrame({\n",
    "            'Feature': X_train_final.columns,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 20 Important Features:\")\n",
    "        print(feature_importances.head(20))\n",
    "    \n",
    "    # Create and return a pipeline with all preprocessing steps\n",
    "    preprocessors = {\n",
    "        'main_preprocessor': preprocessor,\n",
    "        'date_extractor': date_extractor,\n",
    "        'text_extractor': text_extractor,\n",
    "        'component_extractor': component_extractor\n",
    "    }\n",
    "    \n",
    "    return best_model, preprocessors, results\n",
    "\n",
    "def predict_with_complaint_model(model, preprocessors, new_data):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained complaint model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : trained model\n",
    "    preprocessors : dict of fitted preprocessors\n",
    "    new_data : DataFrame with complaint data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with predictions\n",
    "    \"\"\"\n",
    "    # Extract preprocessors\n",
    "    preprocessor = preprocessors['main_preprocessor']\n",
    "    date_extractor = preprocessors['date_extractor']\n",
    "    text_extractor = preprocessors['text_extractor']\n",
    "    component_extractor = preprocessors['component_extractor']\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_preprocessed = preprocessor.transform(new_data)\n",
    "    \n",
    "    # Get feature names from preprocessor\n",
    "    feature_names = []\n",
    "    for name, trans, cols in preprocessor.transformers_:\n",
    "        if name != 'remainder' and trans != 'drop':\n",
    "            if name == 'cat':\n",
    "                # Get feature names for categorical columns after one-hot encoding\n",
    "                cat_features = []\n",
    "                for i, col in enumerate(cols):\n",
    "                    try:\n",
    "                        cat_values = trans.named_steps['onehot'].categories_[i]\n",
    "                        cat_features.extend([f\"{col}_{val}\" for val in cat_values])\n",
    "                    except:\n",
    "                        # Handle case where a column might be empty\n",
    "                        pass\n",
    "                feature_names.extend(cat_features)\n",
    "            else:\n",
    "                # For numeric and binary columns, keep original names\n",
    "                feature_names.extend(cols)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    X_preprocessed_df = pd.DataFrame(X_preprocessed, index=new_data.index, columns=feature_names)\n",
    "    \n",
    "    # Apply other extractors\n",
    "    X_dates = date_extractor.transform(new_data)\n",
    "    X_text = text_extractor.transform(new_data)\n",
    "    X_components = component_extractor.transform(new_data)\n",
    "    \n",
    "    # Combine all features\n",
    "    X_final = pd.concat([X_preprocessed_df, X_dates, X_text, X_components], axis=1)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_final)\n",
    "    prediction_proba = model.predict_proba(X_final)[:, 1]\n",
    "    \n",
    "    # Add predictions to results\n",
    "    result = new_data.copy()\n",
    "    result['predicted_recall'] = predictions\n",
    "    result['recall_probability'] = prediction_proba\n",
    "    \n",
    "    return result\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = pd.read_csv('merged_testing.csv')\n",
    "    \n",
    "    # Only use complaint features\n",
    "    print(\"Using only complaint features to predict recall status...\")\n",
    "    \n",
    "    # Train model\n",
    "    best_model, preprocessors, results = preprocess_and_train_complaint_model(df)\n",
    "    \n",
    "    # Print results summary\n",
    "    for name, result in results.items():\n",
    "        print(f\"\\n{name} Results:\")\n",
    "        print(f\"Test Accuracy: {result['accuracy']:.4f}\")\n",
    "        print(f\"Cross-validation Accuracy: {result['cv_accuracy']:.4f} (±{result['cv_std']:.4f})\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(result['report'])\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(result['confusion_matrix'])\n",
    "    \n",
    "    # Save model for future use\n",
    "    import joblib\n",
    "    joblib.dump(best_model, 'complaint_recall_model.pkl')\n",
    "    joblib.dump(preprocessors, 'complaint_preprocessors.pkl')\n",
    "    \n",
    "    print(\"\\nModel saved successfully!\")\n",
    "    \n",
    "    # Example of making predictions with new data\n",
    "    print(\"\\nExample of prediction with new complaint data:\")\n",
    "    \n",
    "    # Take a small sample from original data for demonstration\n",
    "    new_complaints = df.sample(5)\n",
    "\n",
    "    #adding vehicle_age\n",
    "    current_year = datetime.now().year\n",
    "    new_complaints['vehicle_age'] = current_year - new_complaints['ModelYear']\n",
    "    \n",
    "    # Remove target column if present\n",
    "    if 'recall_status' in new_complaints.columns:\n",
    "        actual_status = new_complaints['recall_status']\n",
    "        new_complaints = new_complaints.drop('recall_status', axis=1)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = predict_with_complaint_model(best_model, preprocessors, new_complaints)\n",
    "    \n",
    "    # Show predictions\n",
    "    print(predictions[['predicted_recall', 'recall_probability']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
